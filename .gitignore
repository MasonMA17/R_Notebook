# History files
.Rhistory
.Rapp.history

# Session Data files
.RData

# User-specific files
.Ruserdata

# Example code in package build process
*-Ex.R

# Output files from R CMD build
/*.tar.gz

# Output files from R CMD check
/*.Rcheck/

# RStudio files
.Rproj.user/

# produced vignettes
vignettes/*.html
vignettes/*.pdf

# OAuth2 token, see https://github.com/hadley/httr/releases/tag/v0.3
.httr-oauth

# knitr and R markdown default cache directories
*_cache/
/cache/

# Temporary files created by R markdown
*.utf8.md
*.knit.md

# R Environment Variables
.Renviron

########################
# create our own functions----一般设置些简单条件创造想要的数据
pacman::p_load(tidyverse,FRACTION,lubridate)
EvenOdd=function(x=2){ if((x%%2)==0){return('Even')} else {return('Odd')}} #奇偶判定
AddFunc = function(x=2,y=2){x+y}
DoubleAdd=function(x=2,y=2){sum=AddFunc(x,y) return(2*sum)}  # Nested function

#Vectorising a Function-----创造函数后，我们往往需要直接判断一列（vector)的情况，而不是单个元素
lapply()
tapply() #avoid using loop
rowMeans()
colMeans() #this two example also could create a vector, notice"s"

x[is.na(x)]=0  #subsetting also can help vectorize a function/ or use lookup
#subsetting example
emp=c('XX','L','DM','SE','SSE')
emp=sample(emp,1000000,replace=T)
designation_employees=function(emp){ 
for(i in 1:length(emp)){ 
if (emp[i]=="DM"){emp[i]="Delivery Manager"
} else if (emp[i]=="L"){emp[i]="Lead"
} else if (emp[i]=="SFE"){emp[i]="Snr.Soft.Eng."
} else if(emp[i]=="SE"){emp[i]="Soft.Eng"
} else {emp[i]="Intern/Trainee"}
}emp}   ##such a heavy job!
#lookup tables
designation_employee3= function(emp){
result= c("DM"="Delivery Manager","L"="Lead",
"SFE"="Snr.Soft.Eng.","SE"="Soft.Eng","xx"="Intern/Trainee")
unname(result[emp])}
benchmark(designation_employees3(emp),replications=10)

##use mapply&vectorize
mapply(Function,...,MoreArgs=NULL,SIMPLIFY=TRUE, USE.NAMES=TRUE) #use mapply
benchmark(mapply(designation_employees,emp))

vec_designation_emp=Vectorize(designation_employees,c('emp'))
benchmark(vec_designation_emp(emp))

#Profiling Code
system.time() #to get the amount of time taken to evaluate an expression 即用来评估效率
install.packages('rbenchmark') #for comparing commands
install.packages('microbenchmark')#for high precision measurements
Rprof()
summaryRprof()  ## for investigate larger chunks of code such as lm()

####### Wide&Long Data------把不同列转为不同行
#pipe fuction-管道函数
%>%  # 将左边的结果传递到右边，一般右边是一个函数
%<>% #也是将左边的结果传递给右边，但是这个操作符会将最后的结果返回给最开始的变量。
%T>% #将左边的结果传递到右边，但返回原始的结果，or是返回上一级结果。
%$%  #左边一般不是变量，而是数据框or列表
%in% #匹配，相当于R中的match函数，左边的元素在右边的向量中是否存在，返回TRUE OR FALSE
#解决column名称中”X2016“分开问题
separate(Year,into=c("x","Year"),sep="X") %>% select(-x) %>% mutate(Year=as.numeric(Year))
f1=read.csv('f1.csv',check.names = F) #another method
#convert to a long data format
gather(df_to_reshape,new_col_key,new_col_value,cols_to_get_values,args(na.rm=FALSE,convert=FALSE))
f1New=gather(f1,'year','count'2:ncol(f1))
head(f1New) 
saplly(f1New,class) #看变量格式，以上的公式里year格式为”character"
f1New=f1%>%gather('year','count',2:ncol(f1),convert=T) #这种情况year格式为“integer”
f1New=f1%>%gather('year','count',2:ncol(f1),convert=T)%>%rename('categories'=Variables)%>%arrange(categories) #重命名+排序

###### Date&Time
Sys.time() #看系统时间
class(Sys.time())
Sys.Date() #只显示年月日
class(Sys.Date)
Sys.timezone() #时区
dates=c("March 27,1987","June 11,1960") #character type
Newdates=as.Date(dates,format="%B,%d,%Y") #转换格式必须要这种格式

timeNow='13-02-2018 9:20' #格式 character
timeNow_tz = dmy_hm(timeNow) #更改成功
timeNow_tz=force_tz(time_tz,"Asia/Kuala_Lumpur") #change timezone
with_tz(time_tz,"Asia/KOLKATA" #convert to desired time zone

year(),month(),wday(),hour(),minute(),second() #显示年月日等

# Work with the inerval
time1=force_tz(time_tz,"Asia/Kuala_Lumpur") #确保时区是相同的
time2=force_tz(time_tz,"Asia/Kuala_Lumpur")
interval=interval(time2,time1)  #create an interval
interval/ddays(1) #days
interval/dhours(1) #hour
interval/weeks(1)  #weeks
interval/dyears(1) #years

####### 练习 两张表“Lekagul Sensor Data" and "coordinates"
pacman::p_load(tidyverse,lubridate)
file1 = 'Lekagul Sensor Data.csv'
file2 = 'coordinates.csv'
data = read.csv(file1, stringsAsFactor = F) #拒绝分类
coordinates = read.csv(file2, stringsAsFactor = F)
lapply(list(data, coordinates), str)
data = data %>%rename(FromTime = Timestamp,FromGate = gate.name,Vehicle = car.type,ID = car.id) #重命名
data$FromTime = dmy_hm(data$FromTime) #correct data types
data[, c('FromGate', 'Vehicle')] = lapply(data[, c('FromGate', 'Vehicle')], factor)#设置分类
data_unique = data%>%distinct() #去除重复值
data_unique = data_unique%>%mutate(SecondsSpent = interval(FromTime,ToTime)/dseconds(1)) #添加新的列并计算时间区间

######画图——ggplot2
#fundamental factors: Data, Mapping(映射),Geometric(几何对象),Scale(标尺),
#Statistics,Coordinante(坐标系统),Layer(图层),Facet(分面),Theme(主题)
p<-ggplot(data=df,mapping=aes(x=col,t=col,shape=col,colour=col))#shape可以将某元素反应到形状上
p+geom_point() #散点图
p<-ggplot(df)
p+geom_point(aes(x=col,t=col,shape=col,colour=col)) #与之前的两步结果一样
ggplot(df)+geom_histogram(aes(x=col,fill=col),position="") #直方图，其中fill为填充颜色，position为分为side by side
ggplot()+geom_bar(aes(x=c(LETTERS[1:3]),y=1:3), stat="identity") #柱状图 stat使按照指定高度画图
ggplot()+geom_density(aes(x=col, colour=col,fill=col)) #密度函数图
ggplot(small)+geom_boxplot(aes(x=cut, y=price,fill=color)) #箱式图
geom_abline 	geom_area 	
geom_bar 		geom_bin2d
geom_blank 		geom_boxplot 	
geom_contour 	geom_crossbar
geom_density 	geom_density2d 	
geom_dotplot 	geom_errorbar
geom_errorbarh 	geom_freqpoly 	
geom_hex 		geom_histogram
geom_hline 		geom_jitter 	
geom_line 		geom_linerange
geom_map 		geom_path 	
geom_point 		geom_pointrange
geom_polygon 	geom_quantile 	
geom_raster 	geom_rect
geom_ribbon 	geom_rug 	
geom_segment 	geom_smooth
geom_step 		geom_text 	
geom_tile 		geom_violin
geom_vline

ggplot(small)+geom_point(aes(x=carat, y=price, shape=cut, colour=color)
)+scale_y_log10()+scale_colour_manual(values=rainbow(7))  #通过scale进行控制

ggplot(small, aes(x=carat, y=price))+geom_point()+scale_y_log10()+stat_smooth() #对原始数据进行计算并加一条回归线
stat_abline       stat_contour      stat_identity     stat_summary
stat_bin          stat_density      stat_qq           stat_summary2d
stat_bin2d        stat_density2d    stat_quantile     stat_summary_hex
stat_bindot       stat_ecdf         stat_smooth       stat_unique
stat_binhex       stat_function     stat_spoke        stat_vline
stat_boxplot      stat_hline        stat_sum          stat_ydensity

ggplot(small)+geom_bar(aes(x=cut, fill=cut))+coord_flip() #coord_flip实现坐标翻转
coord_polar(theta="y") #转换成极坐标 这时会出现饼图
ggplot(small)+geom_bar(aes(x=factor(1), fill=cut))+coord_polar() #靶心图
ggplot(small)+geom_bar(aes(x=clarity, fill=cut))+coord_polar() #风玫瑰图
ggplot(small, aes(x=carat, y=price))+geom_point(aes(colour=cut)
)+scale_y_log10() +facet_wrap(~cut)+stat_smooth()    #facet_wrap 用来分组

ggtitle(),xlab(),ylab() #标题
theme_economist theme_economist_white # 一些系统给到的主题
theme_wsj 	 	theme_excel
theme_few 	 	theme_foundation
theme_igray 	theme_solarized
theme_stata 	theme_tufte

######String manipulations---字符串操作
str_count(x,pattern)
str_delet(x,pattern)
str_locate() str_locate_all()
str_extract()
str_match()
str_replace(x,pattern,replacement)
str_split(x,pattern)  str_split_fixed(x,pattern)

pacman::p_load(tidyverse,rebus,stringdist)
#rebus: SPC:\\s  DOT:.  START:^  END:$  DGT:\\d ALPHA: [:alpha:]
str_to_lower(x)
str_to_upper()
str_squish() #恢复原值
str_detect(x,pattern)

p=or('p','k')%R% END # (?:p|k)$
str_which(x,pattern=o) #显示位置
str_subset(x,pattern=p,negate=T) # negate 表示除了满足P以外的，显示的是元素

#数字为’219 733 8965' ‘234.252.3633'
p1= DGT %R% DGT %R% DGT
p2= DGT %R% DGT %R% DGT
p3= DGT %R% DGT %R% DGT %R%
sep = or(" ”,“.","-","")
ptrn=p1 %R% sep %R% p2 %R% sep %R% p3

str_match(x,pattern=ptrn) # 清洗不干净的数据
sep=char_class('.-')
str_replace_all(nums,pattern=sep,replacement="") #把"."也清洗掉

# values=c('male','M','amle',Female','Femle') 
correct_values= c('M','F')  

adist()  #native packages
amatch()  #Stringdist package
ind= amatch(values,correct_values,maxDist=5)
df=data.frame(initial = values,new_gender = correct_values[ind])  #会将values里的值自动更正为”M“


#####Consistent Data
duplicated(x) 
anyDuplicated()  #返回true值
x[!duplicated(x)] #去除重复值
distinct()
unique()

##Data Imputation
#Impute Missing Data
is.na(),is.nan(),is.infinity()
md.pattern() #mice package-examine the missing pattern ---other packages: mice,mi,Amelia

##例子
cars=read.csv('cars_missing.csv')
original=read.csv('cars.csv')

lapply(list(cars,original),summary)
cols=c('MetColor','Automatic','Doors')
cars[,cols]=lapply(cars[,cols],factor)  #设置分类函数
summary(cars)
#examine missing patterns
pacman::p_load(tidyverse,VIM,mice,Hmisc,DMwR)
library("mi")
library("Amelia")
library("mice")
md.pattern(cars)  #examine the missing pattern
df=as.data.frame(abs(is.na(cars)))
head(df,4)

#用图形探究
aggr(sleep,prop=FALSE,numbers=TRUE) #prop当为TRUE时，显示为缺失值的占比；当为FALSE时，显示为缺失值的数量
#numbers:是否显示数值，默认为FALSE，不显示缺失值的占比或数量
marginplot(sleep[,c('Gest','Dream')],pch = c(20,1),col=c('darkgray','red','blue')) #散点图 红色箱图表示包含插补值之后的分布，灰色箱图表示有效点的分布。红色的空心点是插补之后的点。


#correaltion of missing with othor columns
missing_col=df[which(apply(df,2,sum)>0)]
cor(missing_col)  # look for correlation between columns with missing values to see if there is any further link
cor(cars[,sapply(cars,is.numeric)],missing_col,use = "pairwise.complete.obs") #行是可观测，列为缺失
#MCAR:missing completely at random MAR:missing at random  MNAR: missing not at random

#impute using central tendency
mean_imp=impute(cars$HP,mean)
mode_imp=impute(cars$MetColor,mode)
mice_imput=mice(cars[,!names(cars)%in% "Price"])  #填补missing data 排除price是因为它不适合做自变量进行拟合。
mice_imput
m_out=complete(mice_imput) #for 2nd dataset use complete(mice_imput,2)
dim(m_out)
colMeans(is.na(m_out))  #最后看看有没有缺失值

#accuracy?
actuals=original$HP[is.na(cars$HP)] 
imputed=mean_imp[is.na(cars$HP)]
regr.eval(actuals,imputed) #结果为: mae mse rmse mape
# MSE: mean square error(均方误差）RMSE：前者加上了根号 MAE:mean absolute error（平均绝对误差）
#MAPE: mean absolute percentage error(平均绝对百分比误差）

#for categorical
actuals=original$MetColor[is.na(cars$MetColor)]
imputed=mode_imp[is.na(cars$MetColor)] #the 1st method
imputed=m_out[is.na(cars$MetColor),"MetColor"] ##the 2nd method
prop.table(table(actuals,imputed))  #用来比较真实与预测的误差



####去除outliers
pacman::p_load(tidyverse, DMwR)
cars = read.csv('cars.csv')
ggplot(cars,aes(Price)) + geom_histogram() #when negative distribution,we use 'log', when positive distribution,we use 'square'
ggplot(cars,aes(log(Price))) + geom_histogram() #use log

outliers = filter(cars, (abs(Price - median(Price)) > 2*sd(Price))) #确定outliers
dim(outliers)

cars_new = filter(cars, !(abs(Price - median(Price)) > 2*sd(Price))) #去除outliers
dim(cars_new)

#replace values
qnt = quantile(cars$Price, probs=c(.25, .75), na.rm = T) 
cap = quantile(cars$Price, probs=c(.05, .95), na.rm = T)
Th = 1.5 * IQR(cars$Price, na.rm = T)
cars$cap_price = 0
cars$cap_price = ifelse((cars$Price < (qnt[1] - Th)), cap[1], cars$Price)
cars$cap_price = ifelse((cars$Price > (qnt[2] + Th)), cap[2], cars$Price)  #set upper and lower bounds

cap
qnt

summary(cars)
dim(cars)

options(repr.plot.width=12, repr.plot.height=8) #设置环境变量
linear_model = lm(Price ~ ., data=cars) #线性模型
c_dist = cooks.distance(linear_model)  #库克距离：描述单个样本对整个模型的影响程度，越远影响越大，为outliers，一般为多变量
plot(c_dist, pch="*", cex=2, main="Influential Obs by Cooks distance") #plot cook's distance
abline(h = 4*mean(c_dist, na.rm=T), col="red")  #add cutoff line 
text(x=1:length(c_dist)+1, y=c_dist,labels=ifelse(c_dist>4*mean(c_dist, na.rm=T
),names(c_dist),""), col="red")  # add labels

influential = as.numeric(names(c_dist)[(c_dist > 4*mean(c_dist, na.rm=T))])  # look at influential row numbers
head(cars[influential, ])
dim(cars[influential, ])
dim(cars[-influential, ])

##异常因子检测
str(cars)
scores = lofactor(cars[, sapply(cars, is.numeric)], k =5) #lof :local outlier factor 检测cars中属于numeric的列（因为只对numeric数据有效
plot(density(scores))
outliers = order(scores, decreasing=T)[1:5] #挑出前五的异常值


###################################
##### Time series
pacman::p_load(forecast,tseries,fUnitRoots,tidyverse, lmtest,fastDummies)

data%>%ggplot(aes(x=t,y=Hotel.Occupancy..Y.))+geom_line() #画线图
data$transY=(data$Hotel.Occupancy..Y.)**0.25  ## Change increasing to constant seasonality
data%>%ggplot(aes(x=t,y=transY))+geom_line()

#model1 自动生成的dummy variables
model1 = lm(transY ~ t+Month, data=data)  #create dummy variables automatically as Month is 'character' type
data$predicted = (model1$fitted.values)**4  #get back original Y

ggplot(data, aes(t)) +
geom_line(aes(y = Hotel.Occupancy..Y., colour = "actual")) +
geom_line(aes(y = predicted, colour = "predicted"))          # compare predicted and actual

#model2 手动创造的dummy variables
df = fastDummies::dummy_cols(data)  #create dummy variables beforehand
head(df,4)   #Dec month coefficients are NA, hence 12 months only 11 dummy variables
model2 = lm(transY ~ t+Month_Jan +Month_Feb+Month_Mar+Month_Apr+
Month_May+Month_Jun+Month_Jul+Month_Aug+
Month_Sep+Month_Oct+Month_Nov+Month_Dec, data=df)
summary(model2)

# compare the predictions from the 2 models
cbind(head(model1$fitted.values,5),head(model2$fitted.values,5))  #same result


#######smoothing techniques
SnP = read.csv('SnP500Monthly.csv')
Electricity = read.csv('Electricity.csv')
head(SnP)
head(Electricity)

#plot the graphs
closePrice=SnP$Close
electricityUsed=Electricity$Electricity.Consumed
plot(closePrice,type = "o") # there is trend component, no seasonality
plot(electricityUsed,type = "o") #there is both trend and seasonality

#convert datasets into Time Series data format for processing
tsClosePrice = ts(closePrice, frequency = 12, start = c(1995, 1))
tsClosePrice
tsElectricityUsed = ts(electricityUsed, frequency = 12, start = c(2005, 1))
tsElectricityUsed

#Holts method for trend prediction
tsClosePrice1 <- holt(tsClosePrice,h=12)  #从图中看tsClosePrice1没有SN,选用holts 只考虑trend
autoplot(forecast(tsClosePrice1))
summary(tsClosePrice1)

#Simple Exponential Smoothing. What is the performance? 
SES_tsElectricityUsed <- ses(tsElectricityUsed, alpha = .7, h = 12) #forecast 12 points forward
autoplot(SES_tsElectricityUsed)
summary(SES_tsElectricityUsed)

#Holts method. What is the performance? 
holt_tsElectricityUsed <- holt(tsElectricityUsed,h=12)
autoplot(forecast(holt_tsElectricityUsed))
summary(holt_tsElectricityUsed)

#Holts-Winter method. What is the performance?  ---Electricity 有sn,所以用holts-winter是最好的
holt_winter_tsElectricityUsed <- hw(tsElectricityUsed,h=12)
autoplot(forecast(holt_winter_tsElectricityUsed))
summary(holt_winter_tsElectricityUsed)

#########Decomposition,ARIMA & SARIMA 
series = read.csv('AmtrakBig_raw.csv')
head(series, n=4)
# lag plots
gglagplot(series$Ridership,set.lags = 1:12,do.lines = FALSE) ##能够看出是否有SN，此图里看出SN 周期为12

# check whether the data type is a ts object
is.ts(series$Ridership)
Rider = ts(series$Ridership, frequency = 12, start = c(1991, 1))#converting it into TS object
Rider
is.ts(Rider) # check the datatype again
autoplot(Rider) # There is trend and seasonality

# Using the decomposition function in R 
decompTrainD = decompose(Rider,type = c("additive"))
plot(decompTrainD, lwd=2, col="blue")
#NOTE, currently in R, decomposition does NOT perform prediction automatically using the function#

# AR using ARIMA
arima_100 = arima(Rider, order=c(1,0,0))
arima_100
# forecast the next 12 values
forecasted_values = forecast(arima_100, 12)
forecasted_values
# visual inspection of forecast
autoplot(forecasted_values, lwd=8)

# MA using ARIMA
arima_001 = arima(Rider, order=c(0,0,1))
arima_001
# forecast the next 12 values
forecasted_values = forecast(arima_001, 12)
forecasted_values
# visual inspection of forecast
autoplot(forecasted_values, lwd=8)

#Set train and test range
training = subset(Rider, end=length(Rider)-27)# first 132 values (11 yrs)
test = subset(Rider, start=length(Rider)-26) # start from 133 value till end of series (about 2 years plus)

#perform first order normal differencing
training %>% diff()%>% ggtsdisplay()  #会出现ACF和PACF图
# take an additional seasonal difference
training %>% diff()%>% diff(lag = 12)%>% ggtsdisplay()
# take an additional seasonal difference
training %>% diff()%>% diff(lag = 12)%>% diff(lag = 12)%>% ggtsdisplay()
# makes it worse so we go back to 1 seasonal diff

#let's try AR-1 and MA-1 for both normal and seasonal
model_train_sea = Arima(training, order = c(1,1,1), seasonal=c(1,1,1))
summary(model_train_sea)
coeftest(model_train_sea)

# model tuning
model_train_sea = Arima(training, order = c(1,1,1), seasonal=c(0,1,1))
summary(model_train_sea)
coeftest(model_train_sea)

checkresiduals(model_train_sea)
pacf(model_train_sea$residuals, lag = 36)

# use this model on the test set
model_train_sea %>% forecast(h = 27) %>% autoplot() + autolayer(test) #test is the actual test data

pred_test_sea = Arima(test, model = model_train_sea)
accuracy(pred_test_sea)

# Using Auto-ARIMA to complement mannual tuning

fitAutoArima = auto.arima(training)
fitAutoArima
coeftest(fitAutoArima) # p values
checkresiduals(fitAutoArima)


#######################
###### Neural Network_MLP(multiple layer perceptron

## Method one
#title: "Demo: neural networks for failure prediction"
#output: html_document

library(neuralnet)
library(tictoc)

##### Load data
dfTrain <- read.csv(file = "demo_failue_prediction_train.csv", header=TRUE, stringsAsFactors = FALSE, na.strings = "na" )
dfTest <- read.csv(file = "demo_failue_prediction_test.csv", header=TRUE, stringsAsFactors = FALSE, na.strings = "na" )

#### Pre-process data
dim(dfTrain)

#clean the NA in the original CSV data
dfTrain[is.na(dfTrain)] <- 0    
dfTest[is.na(dfTest)] <- 0

# Scale the Data using the max-min rule
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))} 
dfTrain[2:ncol(dfTrain)] <- lapply(dfTrain[2:ncol(dfTrain)], normalize)
dfTest[2:ncol(dfTest)] <- lapply(dfTest[2:ncol(dfTest)], normalize)

# Clean the NA in the normalization (max(x)==min(x))
dfTrain[is.na(dfTrain)] <- 0
dfTest[is.na(dfTest)] <- 0

# Convert class to be integer
dfTrain$class <- as.integer(dfTrain$class=="pos")
dfTest$class <- as.integer(dfTest$class=="pos")
table(dfTrain$class)
table(dfTest$class)

#### Train neural network
param_nodes_hidden_layer <- c(5,4,2) # No. of nodes at each hidden layer
param_max_iteration <- 1e5           # No. of iterations in training
param_learning_rate <- 0.01          # the learning rate during back propagation
dfTrainSub <- dfTrain

# combine the attributes name for the convenience
names <- colnames(dfTrainSub)
f <- as.formula(paste("class ~", paste(names[!names %in% "class"], collapse = " + ")))

tic("Neural network training")
nnmodel <- neuralnet(f, data = dfTrainSub, hidden=param_nodes_hidden_layer, 
stepmax=param_max_iteration, learningrate = param_learning_rate, linear.output=FALSE) #linear.output=FALSE means this is classification problem, as required by neuralnet package
toc()
plot(nnmodel)
print("NN model training is finished")

### Perform prediction, convert to be binary output
mypredict <- compute(nnmodel, dfTest[,2:ncol(dfTest)])$net.result
mypredict <- sapply(mypredict, round, digits=0)

# Display confusion matrix
results = data.frame(actual = dfTest$class, prediction = mypredict)
table(results)


#### Method two
#Installation guideline
install.packages("keras")
library(keras)
install_keras(tensorflow="1.13.1")
is_keras_available()

# Load data from the CSV file
data_train_orig = read.csv(file="demo_failue_prediction_train.csv", header=TRUE, stringsAsFactors=FALSE, na.strings="na" )
data_test_orig = read.csv(file="demo_failue_prediction_test.csv", header=TRUE, stringsAsFactors=FALSE, na.strings="na" )

# Data cleaning by set 'na' to be zero
data_train_orig[is.na(data_train_orig)] = 0
data_test_orig[is.na(data_test_orig)] = 0
str(data_train_orig)
str(data_test_orig)

x_train_orig = data_train_orig[,2:ncol(data_train_orig)]
y_train_orig = data_train_orig[,1,drop=FALSE]
x_test_orig = data_test_orig[,2:ncol(data_test_orig)]
y_test_orig = data_test_orig[,1,drop=FALSE]             #将自变量和因变量区分到两个表

dim(x_train_orig)
dim(y_train_orig)

# Perfor normalization on neumerical data
x_train = scale(x_train_orig)
col_means_x_train = colMeans(x_train_orig)
col_stds_x_train = apply(x_train_orig, 2, sd)
x_test = scale(x_test_orig, center = col_means_x_train, scale = col_stds_x_train)

# Perform one-hot encoding for category data
y_train = to_categorical(as.matrix(y_train_orig=="pos"), num_classes = 2)
y_test = to_categorical(as.matrix(y_test_orig=="pos"), num_classes = 2)
dim(y_train)
dim(y_test)

# Creating the sequential model
model = keras_model_sequential() %>%   
  layer_dense(units = 10, activation = "relu", input_shape = ncol(x_train)) %>%
  layer_dense(units = 4, activation = "relu") %>%
  layer_dense(units = ncol(y_train), activation = "sigmoid")

summary(model)

y_temp = as.matrix(y_train_orig=="pos")
weight0 = length(y_temp)/(length(y_temp)-sum(y_temp))
weight1 = length(y_temp)/sum(y_temp)

# Train model
compile(model, loss = "binary_crossentropy", optimizer = optimizer_sgd(), metrics = "accuracy")
history = fit(model, data.matrix(x_train), data.matrix(y_train), epochs = 20, batch_size = 100, verbose = 1, class_weight=list("0"=weight0,"1"=weight1))

# Get the prediction score
y_pred = predict(model, x_test)
dim(y_pred)

# Get the prediction class
y_pred_class = predict_classes(model, x_test)

# Generate confusion matrix
y_test_class = as.vector(y_test_orig=="pos")
table(y_test_class, y_pred_class)

sprintf("Accuracy: %.4f %%", mean(y_test_class == y_pred_class)*100)


